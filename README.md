# TOM_Thesis

ABSTRACT

The expansion of Machine Learning (ML) algorithms into diverse decision-making processes has brought a growing impact of algorithmic decisions on peopleâ€™s lives, and with it, the effect of unintended biases in algorithms has increased. These biases can lead to unfair discrimination, or exacerbate existing inequalities among various groups, raising concerns in social, ethical, and legal realms. Therefore, data bias must be carefully considered, particularly in fields that heavily utilize human data.

This paper aims to present a comprehensive framework for detecting, handling, and mitigating bias within the context of ML model development in the context of healthcare clinical predictions, where biological data are used to detect patient hospital readmissions. Conducting an extensive background analysis, the paper explores the sources of bias and their implications in the healthcare ML development cycle and introduces methods to handle and quantify diverse aspects of unfairness utilizing programming tools. The study also provides empirical findings from healthcare predicting modeling 1, spotlighting biases in ML algorithm performance, particularly in the presence of imbalanced data. Through experimentation with various bias mitigation approaches, further assisted by xAI tools, the research emphasized the role of tailored data engineering during the pre-processing phase in effectively mitigating bias, especially in health datasets. 

The objective is to achieve equitable predictions and outcomes for diverse patient groups.
The study underscores the critical need for gaining domain knowledge before diving into the data engineering stage, and advocates for the availability of more diverse and open-source healthcare datasets, encompassing a broad range of patient attributes. 





Key Words: Hospital Decision Making, Fair Machine Learning (ML), Bias in Data and Bias Mitigation, Sensitive Attribute Handling, Diabetes Patients Readmissions, UCI Diabetes Dataset. 




